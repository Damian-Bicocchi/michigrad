{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "949509f2", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from michigrad.engine import Value\n\n# dataset XOR\ndataset = [\n    ([Value(0), Value(0)], Value(0)), # 0 xor 0 = 0\n    ([Value(0), Value(1)], Value(1)), # 0 xor 1 = 1\n    ([Value(1), Value(0)], Value(1)), # 1 xor 0 = 1\n    ([Value(1), Value(1)], Value(0)), # 1 xor 1 = 0\n]", "outputs": []}, {"id": "51ab1f6f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from michigrad.nn import MLP\n\n# Ahora uso un MLP con una capa oculta de 2 neuronas y una neurona de salida.\n# Arquitectura: 2 -> 2 -> 1\n# La capa oculta usa ReLU (por defecto), la salida es lineal.\nmodel = MLP(2, [2, 1])\n\nprint(model)", "outputs": []}, {"id": "5a4126c3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from michigrad.visualize import show_graph\n\nparams = model.parameters()\nlr = 0.1   # learning rate\n\n# Entrenamiento\nfor epoch in range(200):\n\n    # Reseteo loss\n    loss = Value(0)\n\n    # Forward: C\u00e1lculo de L con suma de errores cuadrados\n    for x, y in dataset:\n        yhat = model(x)\n        loss = loss + (y - yhat)**2\n\n    # Pongo todos los gradientes en cero\n    model.zero_grad()\n\n    # Backpropagation\n    loss.backward()\n\n    if epoch == 0:\n      print(\"Grafo despu\u00e9s del backward:\")\n      dot = show_graph(loss, rankdir=\"TB\", format=\"png\")\n      # para que me lo imprima Jupyter a pesar de no ser la \u00faltima l\u00ednea\n      display(dot)\n\n    # Descenso del gradiente (actualiza par\u00e1metros) \n    for p in params:\n        p.data -= lr * p.grad\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}: loss = {loss.data}\")\n\n# Predicciones finales sobre la tabla XOR\nprint(\"\\nPredicciones del modelo entrenado:\")\nfor x, y in dataset:\n    yhat = model(x)\n    salida = 1 if yhat.data > 0.5 else 0\n    print(f\"Entrada { [v.data for v in x] } -> salida cruda = {yhat.data:.4f} | salida binaria = {salida} | esperado = {y.data}\")", "outputs": []}, {"id": "4d4ebd11", "cell_type": "markdown", "source": "### Interpretaci\u00f3n de los resultados del modelo entrenado\n\nEn este ejercicio rehice el problema de la funci\u00f3n XOR usando ahora una red neuronal construida con **Michigrad**. A diferencia del caso anterior, donde trabajaba con una sola neurona lineal, ac\u00e1 utilic\u00e9 un MLP con arquitectura 2\u20132\u20131: dos neuronas en una capa oculta con activaci\u00f3n ReLU y una neurona de salida lineal. El XOR es un ejemplo cl\u00e1sico de problema no linealmente separable, por lo que un modelo puramente lineal no puede resolverlo. La capa oculta introduce una transformaci\u00f3n no lineal del espacio de entrada; cada neurona aprende a activar en distintas regiones del plano definido por las dos entradas. Luego, la neurona de salida combina esas activaciones para aproximar la tabla de verdad del XOR. Despu\u00e9s del entrenamiento, el modelo devuelve valores cercanos a 0 y 1 en los cuatro patrones y, aplicando un umbral sencillo, logra clasificar correctamente todos los casos.", "metadata": {}}]}